---
layout: post
title: 机器学习笔记-基础知识(1)
date: 2018-1-28
categories: blog
tags: [机器学习]
description: 
---
<script type="text/javascript" async src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML"></script>
# 机器学习笔记  基础知识(1)
&emsp;&emsp;我们通常将机器学习算法定义为这样一类算法：对于某给定的**任务T**，在合理的性能度量**方案P**的前提下，某计算机程序可以自主学习**任务T**的**经验E**;随着提供合适、优质、大量的**经验E**，该程序对于**任务T**的性能逐步提高。简单的来讲， 即随着任务的不断执行，经验的累积会带来模型性能的提升。  
&emsp;&emsp;那么机器学习算法有三个重要的对象：**任务T(Task)，经验E(Experience)，性能P(Performance)**。与此对应的是统计学习方法的三要素：**模型(model),策略(strategy)和算法(algorithm)**。   

## 实现机器学习方法的步骤如下：  
1.得到一个有限的**训练数据集合**；  
2.确定包含**所有可能的模型**的假设空间，即学习模型的集合；  
3.确定模型选择的准则，即**学习的策略**；  
4.实现求解的最优模型的算法，即**学习的算法**；  
5.通过学习方法选择**最优模型**；  
6.利用学习的最优模型对**新数据**进行预测或分析。  

## 机器学习的三要素：
### 1 模型
&emsp;&emsp;首先要明确这样一个概念，在机器学习中，输入，输出，参数往往是多维向量，这里就引入了**假设空间**(hypothesis space) 的概念。
模型的假设空间用$$\mathcal{F}$$表示，包含所有可能的条件概率分布或决策函数，**例如**，假设，决策函数为线性函数，那么模型的假设空间就是所有这些线性函数构成的函数集合。  
模型的假设空间可以定义为决策函数的集合：  

$$
\mathcal{F} = \{f|Y=f_{\theta}(X),\theta \in \mathbb{R}^{n}\}
$$   

&emsp;&emsp;其中，X和Y是定义在**输入空间X**和**输出空间Y**上的变量，这时，F通常是由一个参数向量决定的函数族，参数向量$$\theta$$取值于n维欧式空间$$R^n$$，称为参数空间(parameter space)。  

假设空间也可以定义为条件概率的集合：  

$$
\mathcal{F} = \{P|P_{\theta}(Y|X),\theta \in \mathbb{R}^{n}\}
$$  

&emsp;&emsp;其中，X和Y是定义在输入空间X和输出空间Y上的随机变量，这时F通常是有一个参数变量$$\theta$$决定的条件概率分布族。
### 2 策略
&emsp;&emsp;有了模型的假设空间，机器学习接着需要考虑的是，按照什么样的**准则**，**学习或者选择**最优的模型。机器学习的目标就在于在模型的假设空间中选择最优的模型。  
&emsp;&emsp;首先，介绍**损失函数**与**风险函数**的概念。损失函数度量模型一次预测的好坏，风险函数度量平均意义下模型预测的好坏。  
#### 损失函数
&emsp;&emsp;监督学习的问题是在模型的假设空间F中选取**最优的模型**f作为决策函数，对于给定的输入$$X$$，有$$f(x)$$给出相应的预测$$\hat{Y}$$,输出的预测值可能与真实值Y一直，也可能不一致，因此，我们使用损失函数(loss function)或代价函数（cost function）来度量预测错误的程度。损失函数时f（x）和Y的非负实值函数，记做$$L(Y,f(x))$$。  
机器学习常用的损失函数有以下几种：  
1. 0-1损失函数(0-1 loss function)  

$$
L(Y,f(x))=\left\{
\begin{aligned}
1, Y\ne f(x)  \\
0, Y= f(x)
\end{aligned}
\right.
$$  

2. 平方损失函数(quadratic loss function)  

$$
L(Y,f(x))=(Y-f(X))^2
$$

3. 绝对损失函数(absolute loss function)  

$$
L(Y,f(x))=\left|Y-f(x) \right|
$$

4. 对数损失函数(logarithmic loss function)或者对数似然损失函数(log-likelihood loss function)  

$$
L(Y,P(Y|X))=-logP(Y|X)
$$

#### 风险函数
&emsp;&emsp;损失函数值越小，说明模型就越好。由于模型的输入、输出$$(X,Y)$$是随机变量，遵循联合分布$$P(X,Y)$$,所以损失函数的期望为：  

$$
R_{exp}(f)= E_p[L(Y,f(x))] = \int_{\mathcal{X*Y}}L(y,f(x))P(x,y)dxdy
$$  

&emsp;&emsp;这是理论上模型$$f(X)$$关于联合分布$$P(X|Y)$$的平均意义下的损失，称为**风险函数**（risk function）或者**期望损失**(expected loss)。  
由于我们不知道联合分布$$P(X,Y)$$，所以$$R_{exp}(f)$$不能够直接计算，通常情况下，我们给定一个训练数据集：  

$$
T=\left\{(x_1,y_1),(x_2,y_2),...(x_N,y_N) \right\}
$$

模型f(X)关于训练数据集的平均损失称为**经验风险**（empirical risk）或者**经验损失**（empirical loss）。记$$R_{emp}(f)$$:  

$$
R_{emp}(f) = \frac{1}{N}\sum^{N}_{i=1}L(y_i,f(x_i))
$$  

经验风险是模型关于训练样本的平均损失，根据大数定律，当样本容量N趋近于无穷的时候，经验风险趋于期望风险。所以，一个非常自然的想法就是使用经验风险估计期望风险，因此就有了以下两种策略：经验风险最小化和结构风险最小化。
#### 经验风险最小化与结构风险最小化
&emsp;&emsp;在假设空间、损失函数以及训练数据集确定的情况下，经验风险函数即可确定。**经验风险最小化**(expirical risk minimization) 的策略认为：经验风险最小的模型就是最优的模型。根据这一策略，按照经验风险最小化求最优模型就是求解最优化问题：    

$$
min_{f\in \mathcal{F}}\frac{1}{N}\sum^N_{i=1}L(y_i,f(x_i))
$$  

极大似然估计(maximum likelihood estimation)就是经验风险最小化的一个例子：当模型是条件概率分布，损失函数时对数损失函数时，经验风险最小化就等价于极大似然估计。    
&emsp;&emsp;但是，当样本容量很小时，经验风险最小化学习的效果未必就很好，会产生过拟合现象。    

&emsp;&emsp;**结构风险最小化**(structural risk minimization SRM)，是为了防止过拟合而提出来的策略，结构风险在经验风险上加上表示模型复杂度的**正则化项**(regularizer)，或**罚项**(penalty term)，在假设空间、损失函数以及训练数据集确定的情况下，**结构风险**的定义是：   

$$
R_{srm}(f)= \frac{1}{N}\sum^{N}_{i=1}L(y_i,f(x_i)) + \lambda J(f)
$$  

&emsp;&emsp;其中$$J(f)$$为模型的复杂度，是定义在假设空间$$\mathcal{F}$$上的函数，模型f越复杂，复杂度越大，$$\lambda \ge 0$$是系数，用以权衡经验风险和模型复杂度。结构风险小需要经验风险与模型复杂度同时小，结构风险小的模型往往对训练数据以及未知测试数据都有较好的预测。  

结构风险最小化侧率认为结构风险最小的模型是最优的模型，所以求最优模型就是求解最优化问题：  

$$
min_{f\in \mathcal{F}}\frac{1}{N}\sum^N_{i=1}L(y_i,f(x_i))+ \lambda J(f)
$$  

&emsp;&emsp;贝叶斯估计中的最大后验估计(maximum posterior probability)就是结构风险最小化的例子，当模型是条件概率分布，损失函数时对数损失函数，模型复杂度由模型的先验概率表示时，结构风险最小化就等价于最大后验概率估计。

